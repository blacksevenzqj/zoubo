import numpy as np
from sklearn import datasets
import matplotlib as mpl
import matplotlib.pyplot as plt


digits = datasets.load_digits()
x = digits.data
y = digits.target.copy()
print('xçš„é•¿åº¦%i' % len(x), 'yçš„é•¿åº¦%i:' % len(y))

# ä½¿æ•°æ®é›†çš„æ ·æœ¬æ¯”ä¾‹çš„ä¸¥é‡åæ–œï¼Œå˜ä¸º2åˆ†ç±»
y[digits.target == 9] = 1
y[digits.target != 9] = 0

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=666)

from sklearn.linear_model import LogisticRegression

# æ²¡æœ‰ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ­£åˆ™é¡¹å€¼ï¼ˆroc_aucè¯„åˆ†æ ‡å‡†ï¼‰  æˆ–  ç›´æ¥ä½¿ç”¨ LogisticRegressionCV ç±»è‡ªå¸¦çš„äº¤å‰éªŒè¯åŠŸèƒ½
log_reg = LogisticRegression()
log_reg.fit(x_train, y_train) # æ¨¡å‹è®­ç»ƒæ—¶ï¼šæ˜¯ä¸ç”¨é˜ˆå€¼çš„ï¼Œå…·ä½“ç»†çœ‹æŸå¤±å‡½æ•°åŠå…¶æ±‚åå¯¼çš„è¿‡ç¨‹ã€‚

score = log_reg.score(x_test, y_test) # ç›´æ¥æ±‚ å‡†ç¡®ç‡
# print(score)

y_log_predict = log_reg.predict(x_test) # æ±‚ é¢„æµ‹å€¼

y_log_predict_proba = log_reg.predict_proba(x_test)[:,1] # æ±‚ é¢„æµ‹å€¼ æ¦‚ç‡
y_log_predict_proba_predict = np.array(y_log_predict_proba >= 0.5, dtype='int')
# print(len(y_test), np.sum(y_log_predict == y_log_predict_proba_predict)) # å¯è§ é»˜è®¤æ¦‚ç‡ä¸º 0.5

decision_scores = log_reg.decision_function(x_test) # ç†è§£ä¸ºï¼šçº¿æ€§å›å½’ ğœƒx+b çš„ç»“æœ


def TP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) & (y_predict == 1))

# predictTP = TP(y_test, y_log_predict)
# print(predictTP)

def TN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) & (y_predict == 0))

# predictTN = TN(y_test, y_log_predict)
# print(predictTN)

def FP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) & (y_predict == 1))

# predictFP = FP(y_test, y_log_predict)
# print(predictFP)

def FN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) & (y_predict == 0))

# predictFN = FN(y_test, y_log_predict)
# print(predictFN)

# æ··æ·†çŸ©é˜µ
def confusion_matrix(y_true, y_predict):
    return np.array([
        [TN(y_true, y_predict), FP(y_true, y_predict)],
        [FN(y_true, y_predict), TP(y_true, y_predict)],
    ])

# matrix = confusion_matrix(y_test, y_log_predict)
# print(matrix)

# å‡†ç¡®ç‡
def precision_scoreAll(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fp = FP(y_true, y_predict)
    tn = TN(y_true, y_predict)
    fn = FN(y_true, y_predict)
    try:
        return (tp + tn) / (tp + fp + tn + fn)
    except:
        return 0.0

# precisionScoreAll = precision_scoreAll(y_test, y_log_predict)
# print(precisionScoreAll)

# ç²¾å‡†ç‡
def precision_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fp = FP(y_true, y_predict)
    try:
        return tp / (tp + fp)
    except:
        return 0.0

# precisionScore = precision_score(y_test, y_log_predict)
# print(precisionScore)

# å¬å›ç‡
def recall_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fn = FN(y_true, y_predict)
    try:
        return tp / (tp + fn)
    except:
        return 0.0

# recallScore = recall_score(y_test, y_log_predict)
# print(recallScore)

# è°ƒå’Œå¹³å‡å€¼
# F1åˆ†æ•°çš„å…¬å¼ä¸º = 2*æŸ¥å‡†ç‡*æŸ¥å…¨ç‡ / (æŸ¥å‡†ç‡ + æŸ¥å…¨ç‡)
def f1_score_my(precisionScore, recallScore):
    try:
        return 2 * precisionScore * recallScore / (precisionScore + recallScore)
    except:
        return 0.0

# f1Score = f1_score_my(precisionScore, recallScore)
# print(f1Score)

# TPRï¼šå°±æ˜¯å°±æ˜¯å¬å›ç‡
def TPR(y_true, y_predict):
    return recall_score(y_true, y_predict)

# tprScore = TPR(y_test, y_log_predict)
# print(tprScore)

# FPRï¼š
def FPR(y_true, y_predict):
    fp = FP(y_true, y_predict)
    tn = TN(y_true, y_predict)
    try:
        return fp / (fp + tn)
    except:
        return 0.0

# fprScore = FPR(y_test, y_log_predict)
# print(fprScore)


print("=============================================================================================")


# æ··æ·†çŸ©é˜µ
from sklearn.metrics import confusion_matrix
confusionMatrix = confusion_matrix(y_test, y_log_predict)
# print(confusionMatrix)

# å‡†ç¡®ç‡
from sklearn.metrics import accuracy_score
accuracyScore = accuracy_score(y_test, y_log_predict)
# print(accuracyScore)

# ç²¾å‡†ç‡
from sklearn.metrics import precision_score
precisionScore = precision_score(y_test, y_log_predict)
# print(precisionScore)

# å¬å›ç‡
from sklearn.metrics import recall_score
recallScore = recall_score(y_test, y_log_predict)
# print(recallScore)

# F1åˆ†æ•°
from sklearn.metrics import f1_score
f1Score = f1_score(y_test, y_log_predict)
# print(f1Score)

# å¤šåˆ†ç±»ç»¼åˆæŒ‡æ ‡ï¼šåªæœ‰è¿™ä¸ªæŒ‡æ ‡èƒ½è®¡ç®—å¤šåˆ†ç±»ï¼Œä»¥ä¸Šçš„éƒ½æ˜¯è®¡ç®—äºŒåˆ†ç±»çš„
from sklearn.metrics import classification_report
classificationReport = classification_report(y_test, y_log_predict)
# print(classificationReport)


print("=============================================================================================")


# matplotlib å›¾è¡¨ä¸­æ–‡æ˜¾ç¤º
mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = False

# 1.1ã€æ‰‹åŠ¨å¾ªç¯ åˆ›å»ºTPRã€FPRï¼š
precisions = []
recalls = []
f1Scores = []
tprs = []
fprs = []
thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)
for threshold in thresholds:
    # é‡ç‚¹ï¼šå°† decision_scoresï¼ˆçº¿æ€§å›å½’ğœƒx+bçš„ç»“æœï¼‰ å¤§äºï¼ˆæ­£é¢„æµ‹ï¼‰  å…¶è‡ªèº«åŒºé—´ä¸­çš„æŸä¸€ä¸ªé˜ˆå€¼ çš„ç»“æœ è®¾ç½®ä¸º é¢„æµ‹ç»“æœ
    my_predict = np.array(decision_scores >= threshold, dtype='int')
    precisions.append(precision_score(y_test, my_predict))
    recalls.append(recall_score(y_test, my_predict)) # TPR
    f1Scores.append(f1_score(y_test, my_predict))
    tprs.append(TPR(y_test, my_predict)) # recalls
    fprs.append(FPR(y_test, my_predict))
    # print(confusion_matrix(y_test, my_predict))

# print("é˜ˆå€¼ï¼š", thresholds[0:10], 'ç»´åº¦ï¼š', len(thresholds)) # 1056
# print("å‡†ç¡®ç‡ï¼š", precisions[0:10], 'ç»´åº¦ï¼š', len(precisions)) # 1056
# print("å¬å›ç‡ï¼š", recalls[0:10], 'ç»´åº¦ï¼š', len(recalls)) # 1056
# print("F1åˆ†æ•°ï¼š", f1Scores[0:10], 'ç»´åº¦ï¼š', len(f1Scores)) # 1056
# print("TPRï¼š", tprs[0:10], 'ç»´åº¦ï¼š', len(tprs))
# print("FPRï¼š", fprs[0:10], 'ç»´åº¦ï¼š', len(fprs))


# 1.2ã€è‡ªåŠ¨ åˆ›å»ºTPRã€FPR å¾—åˆ° ROCï¼š
from sklearn.metrics import roc_curve
fprs2, tprs2, thresholds2 = roc_curve(y_test, decision_scores) # è‡ªåŠ¨-thresholds2 å’Œ æ‰‹åŠ¨-thresholds æ˜¯ä¸å®Œå…¨ç›¸åŒçš„
fprs3, tprs3, thresholds3 = roc_curve(y_test, y_log_predict_proba)
# print(len(fprs2), sum(fprs2 == fprs3)) # ä¸¤ç§æ–¹å¼ç»“æœç›¸åŒ

# 1.3ã€è‡ªåŠ¨ åˆ›å»ºAUCé¢ç§¯ï¼šArea Under Curve
from sklearn.metrics import roc_auc_score
rocAucScore = roc_auc_score(y_test, decision_scores)
rocAucScore3 = roc_auc_score(y_test, y_log_predict_proba)
# print(rocAucScore, rocAucScore3) # ä¸¤ç§æ–¹å¼ç»“æœç›¸åŒ

# 1.4ã€è®¡ç®—KSå€¼åŠå…¶é˜ˆå€¼ï¼šKS=max(TPR-FPR)
print(len(thresholds), len(recalls), len(fprs))
tempValue = 0.00
maxKsValue = 0.00
maxKsThresholds = 0.00
recallsValue = 0.00
fprsValue = 0.00
for i in np.arange(len(thresholds)):
    tempValue = abs(recalls[i] - fprs[i])
    if tempValue > maxKsValue:
        maxKsValue = tempValue
        maxKsThresholds = thresholds[i]
        recallsValue = recalls[i]
        fprsValue = fprs[i]
print('max(TPR-FPR) = %.6fï¼Œ' % maxKsValue, 'æœ€å¤§é˜ˆå€¼ = %.6f' % maxKsThresholds)
print('max(TPR-FPR) = %.6f' % abs(np.array(recalls) - np.array(fprs)).max())

# 1.5ã€è®¡ç®—F1åˆ†æ•°æœ€å¤§å€¼åŠå…¶é˜ˆå€¼ï¼š
maxF1ScoresValue = max(f1Scores)
maxF1ScoresIndex = f1Scores.index(max(f1Scores)) # ä»å·¦åˆ°å³ï¼šç¬¬ä¸€ä¸ªå‡ºç°çš„æœ€å¤§æ•°çš„ç´¢å¼•
maxF1Thresholds = thresholds[maxF1ScoresIndex]
print('F1æœ€å¤§å€¼ = %.6fï¼Œ' % maxF1ScoresValue, 'æœ€å¤§é˜ˆå€¼ = %.6f' % maxF1Thresholds)
# 1.5.1ã€è®¡ç®—F1åˆ†æ•°æœ€å¤§å€¼é˜ˆå€¼ ä¸ KSå€¼é˜ˆå€¼ è·ç¦»ï¼š
diffValue = maxF1Thresholds - maxKsThresholds
print('F1æœ€å¤§å€¼é˜ˆå€¼ - KSé˜ˆå€¼ = %.6f' % diffValue)


fig = plt.figure(figsize = (24,12))
# 1ã€A1å›¾
# ä»¥é˜ˆå€¼ä¸ºæ¨ªåæ ‡ï¼Œåˆ†åˆ«ä»¥TPRå’ŒFPRçš„å€¼ä¸ºçºµåæ ‡ï¼Œå°±å¯ä»¥ç”»å‡ºä¸¤ä¸ªæ›²çº¿ï¼Œè¿™å°±æ˜¯K-Sæ›²çº¿
ax1 = fig.add_subplot(2,2,1)
plt.plot(thresholds, precisions, color = 'blue', label='ç²¾å‡†ç‡')
plt.plot(thresholds, recalls, color='black', label='å¬å›ç‡/TPR')
plt.plot(thresholds, f1Scores, color='green', label='F1åˆ†æ•°é˜ˆå€¼ = %.6f' % maxF1Thresholds)
plt.plot(thresholds, fprs, color='pink', label='FPR')
plt.plot((maxKsThresholds,maxKsThresholds), (recallsValue,fprsValue), c='r', lw=1.5, ls='--', alpha=0.7, label='KSé˜ˆå€¼ = %.6f' % maxKsThresholds)
plt.plot((maxKsThresholds,maxF1Thresholds), (maxF1ScoresValue,maxF1ScoresValue), c='purple', lw=1.5, ls='-', alpha=0.7, label='(F1-KS)çš„é˜ˆå€¼å·® = %.4f' % diffValue)
plt.legend()  # å›¾ä¾‹
plt.xlabel('é˜ˆå€¼')  # xè½´æ ‡ç­¾
plt.ylabel('ç²¾å‡†ç‡ã€å¬å›ç‡/TPRã€F1åˆ†æ•°ã€FPRã€KS') # yè½´æ ‡ç­¾
plt.title('æ‰‹åŠ¨-é˜ˆå€¼ä¸ç²¾å‡†ç‡ã€å¬å›ç‡/TPRã€F1åˆ†æ•°ã€FPRã€KS=max(TPR-FPR)')  # å›¾å

# 2ã€A2å›¾
ax2 = fig.add_subplot(2,2,2)
plt.plot(precisions, recalls, color='purple', label='P-Ræ›²çº¿')
plt.legend()  # å›¾ä¾‹
plt.xlabel('ç²¾å‡†ç‡')  # xè½´æ ‡ç­¾
plt.ylabel('å¬å›ç‡') # yè½´æ ‡ç­¾
plt.title('æ‰‹åŠ¨-P-Ræ›²çº¿')  # å›¾å

# 3ã€B1å›¾
ax4 = fig.add_subplot(2,2,3)
plt.plot(fprs, tprs, color='purple', label='ROCæ›²çº¿')
plt.legend()  # å›¾ä¾‹
plt.xlabel('FPR')  # xè½´æ ‡ç­¾
plt.ylabel('TPR') # yè½´æ ‡ç­¾
plt.title('æ‰‹åŠ¨-ROCæ›²çº¿')  # å›¾å

# 4ã€B2å›¾
ax4 = fig.add_subplot(2,2,4)
plt.plot(fprs2, tprs2, color='purple', label='AUC=%.3f' % rocAucScore)
plt.plot((0, 1), (0, 1), c='b', lw=1.5, ls='--', alpha=0.7) # æ¨ªè½´fprs2ï¼š0â†’1èŒƒå›´ï¼›ç«–è½´tprs2ï¼š0â†’1èŒƒå›´
plt.xlabel('FPR')  # xè½´æ ‡ç­¾
plt.ylabel('TPR') # yè½´æ ‡ç­¾
plt.grid(b=True)
plt.legend(loc='lower right', fancybox=True, framealpha=0.8, fontsize=14)
plt.title('è‡ªåŠ¨-ROCæ›²çº¿å’ŒAUCå€¼', fontsize=17)

plt.show()