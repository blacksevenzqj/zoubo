import numpy as np
from sklearn import datasets
import matplotlib as mpl
import matplotlib.pyplot as plt


digits = datasets.load_digits()
x = digits.data
y = digits.target.copy()
print('xçš„é•¿åº¦:%i' % len(x), 'yçš„é•¿åº¦:%i' % len(y))

# ä½¿æ•°æ®é›†çš„æ ·æœ¬æ¯”ä¾‹çš„ä¸¥é‡åæ–œ
y[digits.target == 9] = 1
y[digits.target != 9] = 0

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=666)

from sklearn.linear_model import LogisticRegression

# æ²¡æœ‰ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ­£åˆ™é¡¹å€¼ï¼ˆroc_aucè¯„åˆ†æ ‡å‡†ï¼‰  æˆ–  ç›´æ¥ä½¿ç”¨ LogisticRegressionCV ç±»è‡ªå¸¦çš„äº¤å‰éªŒè¯åŠŸèƒ½
log_reg = LogisticRegression()
log_reg.fit(x_train, y_train) # æ¨¡å‹è®­ç»ƒæ—¶ï¼šæ˜¯ä¸ç”¨é˜ˆå€¼çš„ï¼Œå…·ä½“ç»†çœ‹æŸå¤±å‡½æ•°åŠå…¶æ±‚åå¯¼çš„è¿‡ç¨‹ã€‚

score = log_reg.score(x_test, y_test) # ç›´æ¥æ±‚ å‡†ç¡®ç‡
# print(score)

y_log_predict = log_reg.predict(x_test) # æ±‚ é¢„æµ‹å€¼

y_log_predict_proba = log_reg.predict_proba(x_test)[:,1] # æ±‚ é¢„æµ‹å€¼ æ¦‚ç‡

'''
1ã€predictå‡½æ•°ï¼šé»˜è®¤é˜ˆå€¼ä¸º0ï¼Œå¤§äº0çš„ä¸ºä¸€ç±»ã€‚ï¼ˆæ ¹æ®çº¿æ€§å›å½’ ğœƒx+b çš„ç»“æœåˆ¤æ–­ï¼Œæ•™ç¨‹ä¸Šè¯´çš„ï¼ï¼ï¼ï¼‰
2ã€decision_function å‡½æ•°ï¼šæ˜¯çº¿æ€§å›å½’ ğœƒx+b çš„ç»“æœã€‚
'''
decision_scores = log_reg.decision_function(x_test)
# print(decision_scores[:10])
# print(np.min(decision_scores), np.max(decision_scores))


# æ··æ·†çŸ©é˜µ
from sklearn.metrics import confusion_matrix
confusionMatrix = confusion_matrix(y_test, y_log_predict)
# print(confusionMatrix)

# å‡†ç¡®ç‡
from sklearn.metrics import accuracy_score
accuracyScore = accuracy_score(y_test, y_log_predict)
# print(accuracyScore)

# ç²¾å‡†ç‡
from sklearn.metrics import precision_score
precisionScore = precision_score(y_test, y_log_predict)

# å¬å›ç‡
from sklearn.metrics import recall_score
recallScore = recall_score(y_test, y_log_predict)

# f1åˆ†æ•°
from sklearn.metrics import f1_score
f1Score = f1_score(y_test, y_log_predict)

# å¤šåˆ†ç±»ç»¼åˆæŒ‡æ ‡ï¼šåªæœ‰è¿™ä¸ªæŒ‡æ ‡èƒ½è®¡ç®—å¤šåˆ†ç±»ï¼Œä»¥ä¸Šçš„éƒ½æ˜¯è®¡ç®—äºŒåˆ†ç±»çš„ï¼ˆä»¥æ¯ä¸ªç±»åˆ«ä¸ºåŸºå‡†ï¼Œåˆ†åˆ«è®¡ç®— æ¯ä¸ªç±»åˆ«å„è‡ªçš„ ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1 ç­‰æŒ‡æ ‡ï¼‰
from sklearn.metrics import classification_report
classificationReport = classification_report(y_test, y_log_predict)


print("=============================================================================================")


# matplotlib å›¾è¡¨ä¸­æ–‡æ˜¾ç¤º
mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = False

# '''
# 1ã€ç›´æ¥ç”¨ decision_function å‡½æ•°çš„ çº¿æ€§å›å½’ ğœƒx+b çš„ç»“æœ åˆ¤æ–­ï¼š
# y_predict_2 = np.array(decision_scores >= 0, dtype='int')
# print(confusion_matrix(y_test, y_predict_2))

# 1.1.1ã€æ‰‹åŠ¨å¾ªç¯ æ‰¾é˜ˆå€¼ï¼šè‡ªå®šä¹‰é˜€å€¼ï¼š P-Ræ›²çº¿ã€F1åˆ†æ•°
precisions = []
recalls = []
f1Scores = []
thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)
for threshold in thresholds:
    my_predict = np.array(decision_scores >= threshold, dtype='int')
    precisions.append(precision_score(y_test, my_predict))
    recalls.append(recall_score(y_test, my_predict))
    f1Scores.append(f1_score(y_test, my_predict))
    # print(confusion_matrix(my_predict, y_test))

# print("é˜ˆå€¼ï¼š", thresholds[0:10], 'ç»´åº¦ï¼š', len(thresholds)) # 1056
# print("å‡†ç¡®ç‡ï¼š", precisions[0:10], 'ç»´åº¦ï¼š', len(precisions)) # 1056
# print("å¬å›ç‡ï¼š", recalls[0:10], 'ç»´åº¦ï¼š', len(recalls)) # 1056
# print("F1åˆ†æ•°ï¼š", f1Scores[0:10], 'ç»´åº¦ï¼š', len(f1Scores)) # 1056


'''
precision_recall_curve å‡½æ•°ä½¿ç”¨ decision_scores å’Œ y_log_predict_proba è®¡ç®—å¾—åˆ°çš„ ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1 ç›¸åŒï¼Œè€Œ é˜ˆå€¼ä¸åŒï¼›y_log_predict_proba è®¡ç®—çš„é˜ˆå€¼ ä¸èƒ½ä½¿ç”¨ã€‚  
'''
# 1.1.2ã€sklearnçš„å‡½æ•° æ‰¾é˜ˆå€¼ï¼š
from sklearn.metrics import precision_recall_curve # P-Ræ›²çº¿

# è°ƒå’Œå¹³å‡å€¼ F1åˆ†æ•°çš„å…¬å¼ä¸º = 2*æŸ¥å‡†ç‡*æŸ¥å…¨ç‡ / (æŸ¥å‡†ç‡ + æŸ¥å…¨ç‡)
# å›  precision_recall_curve æ²¡æœ‰è¿”å›F1åˆ†æ•°ï¼Œæ‰€ä»¥å†™F1çš„è‡ªå®šä¹‰å‡½æ•°ï¼Œå’Œsklearnçš„f1_scoreå‡½æ•°ç›¸åŒã€‚
def f1_score_my(precisionScore, recallScore):
    try:
        return 2 * precisionScore * recallScore / (precisionScore + recallScore)
    except:
        return 0.0

# 1ã€ precisions2ã€recalls2 å’Œ thresholds çš„shapeç»´åº¦ æ˜¯ç”±å‡½æ•° precision_recall_curve è‡ªå®šä¹‰æ­¥é•¿å†³å®šçš„ã€‚
#  thresholdsçš„ç»´åº¦ æ¯” precisions2ã€recalls2 å°‘1ä¸ªç»´åº¦ã€‚æ‰€ä»¥ precisions2[:-1], recalls2[:-1] è¿›è¡Œè®¡ç®—ã€‚
#  åŸæ–‡å¦‚ä¸‹ï¼š
#  The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold.
#  This ensures that the graph starts on the y axis.
#  æœ€å¤§çš„thresholdå¯¹åº”çš„ ç²¾å‡†ç‡ä¸º1ï¼Œå¬å›ç‡ä¸º0ï¼Œæ‰€ä»¥æ²¡æœ‰ä¿ç•™æœ€å¤§çš„thresholdã€‚
# 2ã€precision_recall_curve å‡½æ•°è®¡ç®—æ—¶ï¼Œæ²¡æœ‰ä» decision_scores ä¸­çš„æœ€å°å€¼å¼€å§‹è®¡ç®—ï¼Œä»å‡½æ•°è‡ªè®¤ä¸ºé‡è¦çš„å€¼å¼€å§‹è®¡ç®—ï¼Œæ‰€ä»¥
# è¿”å›çš„ precisions2, recalls2, thresholds2 ä¹Ÿæ²¡æœ‰å¯¹åº”åˆ° decision_scores ä¸­çš„æœ€å°å€¼ã€‚
precisions2, recalls2, thresholds2 = precision_recall_curve(y_test, decision_scores)
f1Scores2 = f1_score_my(precisions2[:-1], recalls2[:-1])

# print("é˜ˆå€¼ï¼š", thresholds2[0:10], 'ç»´åº¦ï¼š', thresholds2.shape) # 144
# print("å‡†ç¡®ç‡ï¼š", precisions2[0:10], 'ç»´åº¦ï¼š', precisions2.shape) # 145
# print("å¬å›ç‡ï¼š", recalls2[0:10], 'ç»´åº¦ï¼š', recalls2.shape) # 145
# print("F1åˆ†æ•°ï¼š", f1Scores2[0:10], 'ç»´åº¦ï¼š', f1Scores2.shape) # 144


fig = plt.figure(figsize = (24,12))
# 1ã€A1å›¾
ax1 = fig.add_subplot(2,2,1)
plt.plot(thresholds, precisions, color = 'blue', label='ç²¾å‡†ç‡')
plt.plot(thresholds, recalls, color='black', label='å¬å›ç‡')
plt.plot(thresholds, f1Scores, color='green', label='F1åˆ†æ•°')
plt.legend()  # å›¾ä¾‹
plt.xlabel('é˜ˆå€¼')  # xè½´æ ‡ç­¾
plt.ylabel('ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°') # yè½´æ ‡ç­¾
plt.title('æ‰‹åŠ¨-é˜ˆå€¼ä¸ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°')  # å›¾å

# 2ã€A2å›¾
ax2 = fig.add_subplot(2,2,2)
plt.plot(precisions, recalls, color='purple', label='P-Ræ›²çº¿')
plt.legend()  # å›¾ä¾‹
plt.xlabel('ç²¾å‡†ç‡')  # xè½´æ ‡ç­¾
plt.ylabel('å¬å›ç‡') # yè½´æ ‡ç­¾
plt.title('æ‰‹åŠ¨-P-Ræ›²çº¿')  # å›¾å

# 3ã€B1å›¾
ax3 = fig.add_subplot(2,2,3)
plt.plot(thresholds2, precisions2[:-1], color = 'blue', label='ç²¾å‡†ç‡')
plt.plot(thresholds2, recalls2[:-1], color='black', label='å¬å›ç‡')
plt.plot(thresholds2, f1Scores2, color='green', label='F1åˆ†æ•°')
plt.legend()  # å›¾ä¾‹
plt.xlabel('é˜ˆå€¼')  # xè½´æ ‡ç­¾
plt.ylabel('ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°') # yè½´æ ‡ç­¾
plt.title('è‡ªåŠ¨-é˜ˆå€¼ä¸ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°')  # å›¾å

# 4ã€B2å›¾
ax4 = fig.add_subplot(2,2,4)
plt.plot(precisions2[:-1], recalls2[:-1], color='purple', label='P-Ræ›²çº¿')
plt.legend()  # å›¾ä¾‹
plt.xlabel('ç²¾å‡†ç‡')  # xè½´æ ‡ç­¾
plt.ylabel('å¬å›ç‡') # yè½´æ ‡ç­¾
plt.title('è‡ªåŠ¨-P-Ræ›²çº¿')  # å›¾å

plt.show()
# '''



print("============================================================================================================")



# ä»¥ä¸‹ä¸¤ç§è®¡ç®—æ–¹å¼ ç»“æœéå¸¸ç±»ä¼¼ï¼š
'''
# 2.1ã€è½¬æ¢ä¸ºæ¦‚ç‡åˆ¤æ–­ï¼šï¼ˆä½¿ç”¨decision_scoresï¼Œæœ‰é—®é¢˜ï¼‰
def mySigmoid(z): # è®¡ç®—æ¦‚ç‡p
    return 1 / (1 + np.exp(-z))

def myPredict(z, threshold): # æ ¹æ®æ¦‚ç‡påˆ¤æ–­åˆ†ç±»
    return [1 if p >= threshold else 0 for p in mySigmoid(z)]

# my_predict = myPredict(decision_scores, 0.5)
# print(y_log_predict[:10]) # ndarray
# print(my_predict[:10]) # list

# 2.1.1ã€å¾ªç¯æ‰¾é˜ˆå€¼ï¼š
precisions = []
recalls = []
f1Scores = []
thresholds = np.arange(0.02, 0.9, 0.01)
for threshold in thresholds:
    my_predict = myPredict(decision_scores, threshold)
    precisions.append(precision_score(y_test, my_predict))
    recalls.append(recall_score(y_test, my_predict))
    f1Scores.append(f1_score(y_test, my_predict))
    # print(confusion_matrix(my_predict, y_test))

print("é˜ˆå€¼ï¼š", thresholds)
print("å‡†ç¡®ç‡ï¼š", precisions)
print("å¬å›ç‡ï¼š", recalls)
print("F1åˆ†æ•°ï¼š", f1Scores)

fig = plt.figure(figsize = (12,4))
# 1ã€Aå›¾
ax1 = fig.add_subplot(1,2,1)
plt.plot(thresholds, precisions, color = 'blue', label='ç²¾å‡†ç‡')
plt.plot(thresholds, recalls, color='black', label='å¬å›ç‡')
plt.plot(thresholds, f1Scores, color='green', label='F1åˆ†æ•°')
plt.legend()  # å›¾ä¾‹
plt.xlabel('é˜ˆå€¼')  # xè½´æ ‡ç­¾
plt.ylabel('ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°') # yè½´æ ‡ç­¾

# 2ã€Bå›¾
ax2 = fig.add_subplot(1,2,2)
plt.plot(precisions, recalls, color='purple', label='P-Ræ›²çº¿')
plt.legend()  # å›¾ä¾‹
plt.xlabel('ç²¾å‡†ç‡')  # xè½´æ ‡ç­¾
plt.ylabel('å¬å›ç‡') # yè½´æ ‡ç­¾

plt.show()


print("--------------------------------------------------------------------------------------------------------")


# 2.2ã€è½¬æ¢ä¸ºæ¦‚ç‡åˆ¤æ–­ï¼šï¼ˆä½¿ç”¨y_log_predict_probaï¼Œæœ‰é—®é¢˜ï¼‰
# 2.2.1ã€å¾ªç¯æ‰¾é˜ˆå€¼ï¼š
precisions = []
recalls = []
f1Scores = []
thresholds = np.arange(0.01, 0.99, 0.01)
for threshold in thresholds:
    my_predict = np.array(y_log_predict_proba >= threshold, dtype='int')
    precisions.append(precision_score(y_test, my_predict))
    recalls.append(recall_score(y_test, my_predict))
    f1Scores.append(f1_score(y_test, my_predict))
    # print(confusion_matrix(my_predict, y_test))

print("é˜ˆå€¼ï¼š", thresholds)
print("å‡†ç¡®ç‡ï¼š", precisions)
print("å¬å›ç‡ï¼š", recalls)
print("F1åˆ†æ•°ï¼š", f1Scores)

fig = plt.figure(figsize = (12,4))
# 1ã€Aå›¾
ax1 = fig.add_subplot(1,2,1)
plt.plot(thresholds, precisions, color = 'blue', label='ç²¾å‡†ç‡')
plt.plot(thresholds, recalls, color='black', label='å¬å›ç‡')
plt.plot(thresholds, f1Scores, color='green', label='F1åˆ†æ•°')
plt.legend()  # å›¾ä¾‹
plt.xlabel('é˜ˆå€¼')  # xè½´æ ‡ç­¾
plt.ylabel('ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°') # yè½´æ ‡ç­¾

# 2ã€Bå›¾
ax2 = fig.add_subplot(1,2,2)
plt.plot(precisions, recalls, color='purple', label='P-Ræ›²çº¿')
plt.legend()  # å›¾ä¾‹
plt.xlabel('ç²¾å‡†ç‡')  # xè½´æ ‡ç­¾
plt.ylabel('å¬å›ç‡') # yè½´æ ‡ç­¾

plt.show()
'''