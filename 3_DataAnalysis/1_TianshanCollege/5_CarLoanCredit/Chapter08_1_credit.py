
# coding: utf-8

# # é€»è¾‘å›å½’
#ä¿¡ç”¨é£é™©å»ºæ¨¡æ¡ˆä¾‹
##æ•°æ®è¯´æ˜ï¼šæœ¬æ•°æ®æ˜¯ä¸€ä»½æ±½è½¦è´·æ¬¾è¿çº¦æ•°æ®
##åç§°---ä¸­æ–‡å«ä¹‰
##application_id---ç”³è¯·è€…ID
##account_number---å¸æˆ·å·
##bad_ind---æ˜¯å¦è¿çº¦
##vehicle_year---æ±½è½¦è´­ä¹°æ—¶é—´
##vehicle_make---æ±½è½¦åˆ¶é€ å•†
##bankruptcy_ind---æ›¾ç»ç ´äº§æ ‡è¯†
##tot_derog---äº”å¹´å†…ä¿¡ç”¨ä¸è‰¯äº‹ä»¶æ•°é‡(æ¯”å¦‚æ‰‹æœºæ¬ è´¹æ¶ˆå·)
##tot_tr---å…¨éƒ¨å¸æˆ·æ•°é‡
##age_oldest_tr---æœ€ä¹…è´¦å·å­˜ç»­æ—¶é—´(æœˆ)
##tot_open_tr---åœ¨ä½¿ç”¨å¸æˆ·æ•°é‡
##tot_rev_tr---åœ¨ä½¿ç”¨å¯å¾ªç¯è´·æ¬¾å¸æˆ·æ•°é‡(æ¯”å¦‚ä¿¡ç”¨å¡)
##tot_rev_debt---åœ¨ä½¿ç”¨å¯å¾ªç¯è´·æ¬¾å¸æˆ·ä½™é¢(æ¯”å¦‚ä¿¡ç”¨å¡æ¬ æ¬¾)
##tot_rev_line---å¯å¾ªç¯è´·æ¬¾å¸æˆ·é™é¢(ä¿¡ç”¨å¡æˆæƒé¢åº¦)
##rev_util---å¯å¾ªç¯è´·æ¬¾å¸æˆ·ä½¿ç”¨æ¯”ä¾‹(ä½™é¢/é™é¢)
##fico_score---FICOæ‰“åˆ†
##purch_price---æ±½è½¦è´­ä¹°é‡‘é¢(å…ƒ)
##msrp---å»ºè®®å”®ä»·
##down_pyt---åˆ†æœŸä»˜æ¬¾çš„é¦–æ¬¡äº¤æ¬¾
##loan_term---è´·æ¬¾æœŸé™(æœˆ)
##loan_amt---è´·æ¬¾é‡‘é¢
##ltv---è´·æ¬¾é‡‘é¢/å»ºè®®å”®ä»·*100
##tot_income---æœˆå‡æ”¶å…¥(å…ƒ)
##veh_mileage---è¡Œä½¿å†ç¨‹(Mile)
##used_ind---æ˜¯å¦äºŒæ‰‹è½¦
##weight---æ ·æœ¬æƒé‡


# In[1]:

import os
import numpy as np
from scipy import stats
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt

#pd.set_option('display.max_columns', None)
os.chdir(r"E:\soft\Anaconda\Anaconda_Python3.6_code\data_analysis\TianshanCollege\5_CarLoanCredit")

# å¯¼å…¥æ•°æ®å’Œæ•°æ®æ¸…æ´—
# In[5]:
accepts = pd.read_csv('accepts.csv').dropna()

##è¡ç”Ÿå˜é‡:
def divMy(x,y):
    if x==np.nan or y==np.nan:
        return np.nan
    elif float(y) == 0 :
        return -1
    else:
        return float(x) / float(y)
divMy('1','0.0')

#%%
accepts[["tot_rev_line","tot_income"]].head()
# å†å²è´Ÿå€ºæ”¶å…¥æ¯”:tot_rev_line/tot_income
accepts["dti_hist"]=accepts[["tot_rev_line","tot_income"]].apply(lambda x:divMy(x[0],x[1]),axis = 1)
# æœ¬æ¬¡æ–°å¢è´Ÿå€ºæ”¶å…¥æ¯”:loan_amt/tot_income
accepts["dti_mew"]=accepts[["loan_amt","tot_income"]].apply(lambda x:divMy(x[0],x[1]),axis = 1)
# æœ¬æ¬¡è´·æ¬¾é¦–ä»˜æ¯”ä¾‹:down_pyt/loan_amt
accepts["fta"]=accepts[["down_pyt","loan_amt"]].apply(lambda x:divMy(x[0],x[1]),axis = 1)
# æ–°å¢å€ºåŠ¡æ¯”:loan_amt/tot_rev_debt
accepts["nth"]=accepts[["loan_amt","tot_rev_debt"]].apply(lambda x:divMy(x[0],x[1]),axis = 1)
# æ–°å¢å€ºåŠ¡é¢åº¦æ¯”:loan_amt/tot_rev_line
accepts["nta"]=accepts[["loan_amt","tot_rev_line"]].apply(lambda x:divMy(x[0],x[1]),axis = 1)

accepts.head()


# ä¸€ã€åˆ†ç±»å˜é‡çš„ç›¸å…³å…³ç³»
# äº¤å‰è¡¨
# In[6]:
cross_table = pd.crosstab(accepts.used_ind,accepts.bad_ind, margins=True)
#cross_table = pd.crosstab(accepts.bankruptcy_ind,accepts.bad_ind, margins=True)

cross_table

# åˆ—è”è¡¨æ£€éªŒ
# åŸå‡è®¾ï¼šä¸ç›¸å…³ï¼› å¤‡é€‰å‡è®¾ï¼šç›¸å…³ã€‚
# In[7]:
#W[0,0] / è¡Œ1æ€»è®¡
#W[0,1] / è¡Œ1æ€»è®¡
#W[1,0] / è¡Œ2æ€»è®¡
#W[1,1] / è¡Œ2æ€»è®¡
def percConvert(ser):
    return ser/float(ser[-1])

cross_table.apply(percConvert, axis=1) # cross_tableå€¼ä¸å˜

# In[8]:
# å¡æ–¹æ£€éªŒ
# chisq å¡æ–¹å€¼
# p-value å¡æ–¹å€¼å¯¹åº”çš„ æ˜¾è‘—åº¦Î±ï¼Œç”¨p-valueè¡¨ç¤ºã€‚ æ˜¾è‘—ä»¥å¦ çš„è¡¡é‡æ ‡å‡†ï¼Œå’Œ ä¸¤æ ·æœ¬Tæ£€éªŒ çš„På€¼æ˜¯ä¸€æ ·çš„æ„æ€ã€‚
# expected_freq å¡æ–¹æ£€éªŒçš„ æœŸæœ›é¢‘ç‡ = (è¡Œåˆè®¡ * åˆ—åˆè®¡) / æ€»å’Œ
print(cross_table.iloc[:2, :2])
print('''chisq = %6.4f 
p-value = %6.4f
dof = %i 
expected_freq = %s'''  %stats.chi2_contingency(cross_table.iloc[:2, :2]))



# é€»è¾‘å›å½’
# In[9]:
# age_oldest_tr:æœ€ä¹…è´¦å·å­˜ç»­æ—¶é—´(æœˆ)
accepts.plot(x='age_oldest_tr', y='bad_ind', kind='scatter')

# éšæœºæŠ½æ ·ï¼Œå»ºç«‹è®­ç»ƒé›†ä¸æµ‹è¯•é›†
# In[10]:
# copy() æµ…æ‹·è´
train = accepts.sample(frac=0.7, random_state=1234).copy() # 70%çš„æ ·æœ¬è®­ç»ƒ
test = accepts[~ accepts.index.isin(train.index)].copy() # ~è¡¨ç¤ºéï¼Œé‚£å°±æ˜¯å‰©ä¸‹30%çš„æ ·æœ¬
print(' è®­ç»ƒé›†æ ·æœ¬é‡: %i \n æµ‹è¯•é›†æ ·æœ¬é‡: %i' %(len(train), len(test)))

# In[11]:
# statsmodelsåŒ…ä¸­çš„é€»è¾‘å›å½’
lg = smf.glm('bad_ind ~ age_oldest_tr', data=train, 
             family=sm.families.Binomial(sm.families.links.logit)).fit()
lg.summary() # é€»è¾‘å›å½’ä¸ç”¨çœ‹æŒ‡æ ‡ï¼Œéƒ½æ²¡æœ‰ç”¨

# é¢„æµ‹
# In[19]:
# å¾—åˆ°çš„æ˜¯æ¦‚ç‡ï¼Œè€Œéæœ€ç»ˆçš„0/1ç»“æœï¼ŒåŒºåˆ«äºsklearnä¸­çš„decision_functionçš„ğœƒx+bç»“æœ
train['proba'] = lg.predict(train)
test['proba'] = lg.predict(test)

test['proba'].head(10)

# In[12]:
# æ¨¡å‹è¯„ä¼°
# è®¾å®šé˜ˆå€¼
# In[20]:
test['prediction'] = (test['proba'] > 0.3).astype('int')

# æ··æ·†çŸ©é˜µ
# In[22]:
pd.crosstab(test.bad_ind, test.prediction, margins=True)

# - è®¡ç®—å‡†ç¡®ç‡
# In[23]:
acc = sum(test['prediction'] == test['bad_ind']) /np.float(len(test))
print('The accurancy is %.2f' %acc)

# In[25]:
for i in np.arange(0.02, 0.3, 0.02):
    prediction = (test['proba'] > i).astype('int')
    confusion_matrix = pd.crosstab(prediction,test.bad_ind, margins = True)
    precision = confusion_matrix.ix[0, 0] /confusion_matrix.ix['All', 0]
    recall = confusion_matrix.ix[0, 0] / confusion_matrix.ix[0, 'All']
    Specificity = confusion_matrix.ix[1, 1] /confusion_matrix.ix[1,'All']
    f1_score = 2 * (precision * recall) / (precision + recall)
    print('threshold: %s, precision: %.2f, recall:%.2f ,Specificity:%.2f , f1_score:%.2f'%(i, precision, recall, Specificity,f1_score))


# - ç»˜åˆ¶ROCæ›²çº¿
# In[27]:
import sklearn.metrics as metrics

fpr_test, tpr_test, th_test = metrics.roc_curve(test.bad_ind, test.proba)
fpr_train, tpr_train, th_train = metrics.roc_curve(train.bad_ind, train.proba)

plt.figure(figsize=[3, 3])
plt.plot(fpr_test, tpr_test, 'b--')
plt.plot(fpr_train, tpr_train, 'r-')
plt.title('ROC curve')
plt.show()

# In[28]:
print('AUC = %.4f' %metrics.auc(fpr_test, tpr_test))


# åŒ…å«åˆ†ç±»é¢„æµ‹å˜é‡çš„é€»è¾‘å›å½’
#%%
formula = '''bad_ind ~ C(used_ind)'''
lg_m = smf.glm(formula=formula, data=train, 
             family=sm.families.Binomial(sm.families.links.logit)).fit()
lg_m.summary()

# In[14]:
#- å¤šå…ƒé€»è¾‘å›å½’
# å‘å‰æ³•
def forward_select(data, response):
    remaining = set(data.columns)
    remaining.remove(response)
    selected = []
    current_score, best_new_score = float('inf'), float('inf')
    while remaining:
        aic_with_candidates=[]
        for candidate in remaining:
            formula = "{} ~ {}".format(
                response,' + '.join(selected + [candidate]))
            aic = smf.glm(
                formula=formula, data=data, 
                family=sm.families.Binomial(sm.families.links.logit)
            ).fit().aic
            aic_with_candidates.append((aic, candidate))
        aic_with_candidates.sort(reverse=True)
        best_new_score, best_candidate=aic_with_candidates.pop()
        if current_score > best_new_score: 
            remaining.remove(best_candidate)
            selected.append(best_candidate)
            current_score = best_new_score
            print ('aic is {},continuing!'.format(current_score))
        else:        
            print ('forward selection over!')
            break
            
    formula = "{} ~ {} ".format(response,' + '.join(selected))
    print('final formula is {}'.format(formula))
    model = smf.glm(
        formula=formula, data=data, 
        family=sm.families.Binomial(sm.families.links.logit)
    ).fit()
    return(model)


# In[16]:
#åªæœ‰è¿ç»­å˜é‡å¯ä»¥è¿›è¡Œå˜é‡ç­›é€‰ï¼Œåˆ†ç±»å˜é‡éœ€è¦è¿›è¡ŒWOEè½¬æ¢æ‰å¯ä»¥è¿›è¡Œå˜é‡ç­›é€‰
candidates = ['bad_ind','tot_derog','age_oldest_tr','tot_open_tr','rev_util','fico_score','loan_term','ltv',
              'veh_mileage','dti_hist','dti_mew','fta','nth','nta']
data_for_select = train[candidates]

lg_m1 = forward_select(data=data_for_select, response='bad_ind')
lg_m1.summary()


# Seemingly wrong when using 'statsmmodels.stats.outliers_influence.variance_inflation_factor'
# æ–¹å·®è†¨èƒ€å› å­ å…±çº¿æ€§æ£€éªŒï¼š ç‰¹å¾å‘é‡ é€‰æ‹©å®Œä¹‹åï¼Œè¿˜è¦è¿›è¡Œ å…±çº¿æ€§æ£€éªŒ
# In[17]:
def vif(df, col_i):
    from statsmodels.formula.api import ols
    
    cols = list(df.columns)
    cols.remove(col_i)
    cols_noti = cols
    formula = col_i + '~' + '+'.join(cols_noti)
    r2 = ols(formula, df).fit().rsquared
    return 1. / (1. - r2)

# In[18]:
candidates = ['bad_ind','fico_score','ltv','age_oldest_tr','tot_derog','nth','tot_open_tr','veh_mileage','rev_util']
exog = train[candidates].drop(['bad_ind'], axis=1)

for i in exog.columns:
    print(i, '\t', vif(df=exog, col_i=i))

#%%
train['proba'] = lg_m1.predict(train)
test['proba'] = lg_m1.predict(test)
import sklearn.metrics as metrics

fpr_test, tpr_test, th_test = metrics.roc_curve(test.bad_ind, test.proba)
fpr_train, tpr_train, th_train = metrics.roc_curve(train.bad_ind, train.proba)

plt.figure(figsize=[3, 3])
plt.plot(fpr_test, tpr_test, 'b--')
plt.plot(fpr_train, tpr_train, 'r-')
plt.title('ROC curve')
plt.show()

# In[28]:
print('AUC = %.4f' %metrics.auc(fpr_test, tpr_test))

#%%
#ç›®å‰vehicle_yearã€vehicle_makeã€bankruptcy_indã€used_indè¿™äº›åˆ†ç±»å˜é‡æ— æ³•é€šè¿‡é€æ­¥å˜é‡ç­›é€‰æ³•
#è§£å†³æ–¹æ¡ˆï¼š
#1ã€é€ä¸€æ ¹æ®æ˜¾è‘—æ€§æµ‹è¯•
#2ã€ä½¿ç”¨å†³ç­–æ ‘ç­‰æ–¹æ³•ç­›é€‰å˜é‡ï¼Œä½†æ˜¯å¤šåˆ†ç±»å˜é‡éœ€è¦äº‹å…ˆè¿›è¡Œå˜é‡æ¦‚åŒ–
#3ã€ä½¿ç”¨WOEè½¬æ¢ï¼Œå¤šåˆ†ç±»å˜é‡ä¹Ÿéœ€è¦äº‹å…ˆè¿›è¡Œæ¦‚åŒ–ï¼Œä½¿ç”¨scorecardpyåŒ…ä¸­çš„woeç®—æ³•å¯ä»¥è‡ªåŠ¨è¿›è¡Œæ¦‚åŒ–
# ä½¿ç”¨ç¬¬ä¸€ç§æ–¹æ³•
#formula = '''bad_ind ~ fico_score+ltv+age_oldest_tr+tot_derog+nth+tot_open_tr+veh_mileage+rev_util+C(used_ind)+C(vehicle_year)+C(bankruptcy_ind)'''
formula = '''bad_ind ~ fico_score+ltv+age_oldest_tr+tot_derog+nth+tot_open_tr+veh_mileage+rev_util+C(bankruptcy_ind)'''
lg_m = smf.glm(formula=formula, data=train, 
             family=sm.families.Binomial(sm.families.links.logit)).fit()
lg_m.summary()

#%%



