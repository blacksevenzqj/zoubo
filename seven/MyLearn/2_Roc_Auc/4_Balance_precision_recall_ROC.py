import numpy as np
from sklearn import datasets
import matplotlib as mpl
import matplotlib.pyplot as plt


digits = datasets.load_digits()
x = digits.data
y = digits.target.copy()
print('xçš„é•¿åº¦%i' % len(x), 'yçš„é•¿åº¦%i:' % len(y))

# ä½¿æ•°æ®é›†çš„æ ·æœ¬æ¯”ä¾‹çš„ä¸¥é‡åæ–œï¼Œå˜ä¸º2åˆ†ç±»
y[digits.target == 9] = 1
y[digits.target != 9] = 0

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=666)

from sklearn.linear_model import LogisticRegression

# æ²¡æœ‰ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ­£åˆ™é¡¹å€¼ï¼ˆroc_aucè¯„åˆ†æ ‡å‡†ï¼‰  æˆ–  ç›´æ¥ä½¿ç”¨ LogisticRegressionCV ç±»è‡ªå¸¦çš„äº¤å‰éªŒè¯åŠŸèƒ½
log_reg = LogisticRegression()
log_reg.fit(x_train, y_train)

score = log_reg.score(x_test, y_test) # ç›´æ¥æ±‚ å‡†ç¡®ç‡
# print(score)
y_log_predict = log_reg.predict(x_test) # æ±‚ é¢„æµ‹å€¼

decision_scores = log_reg.decision_function(x_test) # çº¿æ€§å›å½’ ğœƒx+b çš„ç»“æœ


def TP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) & (y_predict == 1))

# predictTP = TP(y_test, y_log_predict)
# print(predictTP)

def TN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) & (y_predict == 0))

# predictTN = TN(y_test, y_log_predict)
# print(predictTN)

def FP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) & (y_predict == 1))

# predictFP = FP(y_test, y_log_predict)
# print(predictFP)

def FN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) & (y_predict == 0))

# predictFN = FN(y_test, y_log_predict)
# print(predictFN)

# æ··æ·†çŸ©é˜µ
def confusion_matrix(y_true, y_predict):
    return np.array([
        [TN(y_true, y_predict), FP(y_true, y_predict)],
        [FN(y_true, y_predict), TP(y_true, y_predict)],
    ])

# matrix = confusion_matrix(y_test, y_log_predict)
# print(matrix)

# å‡†ç¡®ç‡
def precision_scoreAll(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fp = FP(y_true, y_predict)
    tn = TN(y_true, y_predict)
    fn = FN(y_true, y_predict)
    try:
        return (tp + tn) / (tp + fp + tn + fn)
    except:
        return 0.0

# precisionScoreAll = precision_scoreAll(y_test, y_log_predict)
# print(precisionScoreAll)

# ç²¾å‡†ç‡
def precision_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fp = FP(y_true, y_predict)
    try:
        return tp / (tp + fp)
    except:
        return 0.0

# precisionScore = precision_score(y_test, y_log_predict)
# print(precisionScore)

# å¬å›ç‡
def recall_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fn = FN(y_true, y_predict)
    try:
        return tp / (tp + fn)
    except:
        return 0.0

# recallScore = recall_score(y_test, y_log_predict)
# print(recallScore)

# è°ƒå’Œå¹³å‡å€¼
# F1åˆ†æ•°çš„å…¬å¼ä¸º = 2*æŸ¥å‡†ç‡*æŸ¥å…¨ç‡ / (æŸ¥å‡†ç‡ + æŸ¥å…¨ç‡)
def f1_score_my(precisionScore, recallScore):
    try:
        return 2 * precisionScore * recallScore / (precisionScore + recallScore)
    except:
        return 0.0

# f1Score = f1_score_my(precisionScore, recallScore)
# print(f1Score)

# TPRï¼šå°±æ˜¯å°±æ˜¯å¬å›ç‡
def TPR(y_true, y_predict):
    return recall_score(y_true, y_predict)

# tprScore = TPR(y_test, y_log_predict)
# print(tprScore)

# FPRï¼š
def FPR(y_true, y_predict):
    fp = FP(y_true, y_predict)
    tn = TN(y_true, y_predict)
    try:
        return fp / (fp + tn)
    except:
        return 0.0

# fprScore = FPR(y_test, y_log_predict)
# print(fprScore)


print("=============================================================================================")


# æ··æ·†çŸ©é˜µ
from sklearn.metrics import confusion_matrix
confusionMatrix = confusion_matrix(y_test, y_log_predict)
# print(confusionMatrix)

# å‡†ç¡®ç‡
from sklearn.metrics import accuracy_score
accuracyScore = accuracy_score(y_test, y_log_predict)
# print(accuracyScore)

# ç²¾å‡†ç‡
from sklearn.metrics import precision_score
precisionScore = precision_score(y_test, y_log_predict)
# print(precisionScore)

# å¬å›ç‡
from sklearn.metrics import recall_score
recallScore = recall_score(y_test, y_log_predict)
# print(recallScore)

# F1åˆ†æ•°
from sklearn.metrics import f1_score
f1Score = f1_score(y_test, y_log_predict)
# print(f1Score)

# å¤šåˆ†ç±»ç»¼åˆæŒ‡æ ‡
from sklearn.metrics import classification_report
classificationReport = classification_report(y_test, y_log_predict)
# print(classificationReport)


print("=============================================================================================")


# matplotlib å›¾è¡¨ä¸­æ–‡æ˜¾ç¤º
mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = False

# 1.1ã€æ‰‹åŠ¨å¾ªç¯ åˆ›å»ºTPRã€FPRï¼š
precisions = []
recalls = []
f1Scores = []
tprs = []
fprs = []
thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)
for threshold in thresholds:
    my_predict = np.array(decision_scores >= threshold, dtype='int')
    precisions.append(precision_score(y_test, my_predict))
    recalls.append(recall_score(y_test, my_predict))
    f1Scores.append(f1_score(y_test, my_predict))
    tprs.append(TPR(y_test, my_predict))
    fprs.append(FPR(y_test, my_predict))
    # print(confusion_matrix(y_test, my_predict))

# print("é˜ˆå€¼ï¼š", thresholds[0:10], 'ç»´åº¦ï¼š', len(thresholds)) # 1056
# print("å‡†ç¡®ç‡ï¼š", precisions[0:10], 'ç»´åº¦ï¼š', len(precisions)) # 1056
# print("å¬å›ç‡ï¼š", recalls[0:10], 'ç»´åº¦ï¼š', len(recalls)) # 1056
# print("F1åˆ†æ•°ï¼š", f1Scores[0:10], 'ç»´åº¦ï¼š', len(f1Scores)) # 1056
# print("TPRï¼š", tprs[0:10], 'ç»´åº¦ï¼š', len(tprs))
# print("FPRï¼š", fprs[0:10], 'ç»´åº¦ï¼š', len(fprs))


# 1.2ã€è‡ªåŠ¨ åˆ›å»ºTPRã€FPR å¾—åˆ° ROCï¼š
from sklearn.metrics import roc_curve

fprs2, tprs2, thresholds2 = roc_curve(y_test, decision_scores)

# 1.3ã€è‡ªåŠ¨ åˆ›å»ºAUCé¢ç§¯ï¼šArea Under Curve
from sklearn.metrics import roc_auc_score

rocAucScore = roc_auc_score(y_test, decision_scores)


fig = plt.figure(figsize = (24,12))
# 1ã€A1å›¾
ax1 = fig.add_subplot(2,2,1)
plt.plot(thresholds, precisions, color = 'blue', label='ç²¾å‡†ç‡')
plt.plot(thresholds, recalls, color='black', label='å¬å›ç‡')
plt.plot(thresholds, f1Scores, color='green', label='F1åˆ†æ•°')
plt.legend()  # å›¾ä¾‹
plt.xlabel('é˜ˆå€¼')  # xè½´æ ‡ç­¾
plt.ylabel('ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°') # yè½´æ ‡ç­¾
plt.title('æ‰‹åŠ¨-é˜ˆå€¼ä¸ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°')  # å›¾å

# 2ã€A2å›¾
ax2 = fig.add_subplot(2,2,2)
plt.plot(precisions, recalls, color='purple', label='P-Ræ›²çº¿')
plt.legend()  # å›¾ä¾‹
plt.xlabel('ç²¾å‡†ç‡')  # xè½´æ ‡ç­¾
plt.ylabel('å¬å›ç‡') # yè½´æ ‡ç­¾
plt.title('æ‰‹åŠ¨-P-Ræ›²çº¿')  # å›¾å

# 3ã€B1å›¾
ax4 = fig.add_subplot(2,2,3)
plt.plot(fprs, tprs, color='purple', label='ROCæ›²çº¿')
plt.legend()  # å›¾ä¾‹
plt.xlabel('FPR')  # xè½´æ ‡ç­¾
plt.ylabel('TPR') # yè½´æ ‡ç­¾
plt.title('æ‰‹åŠ¨-ROCæ›²çº¿')  # å›¾å

# 4ã€B2å›¾
ax4 = fig.add_subplot(2,2,4)
plt.plot(fprs2, tprs2, color='purple', label='AUC=%.3f' % rocAucScore)
plt.plot((0, 1), (0, 1), c='b', lw=1.5, ls='--', alpha=0.7)
plt.xlabel('FPR')  # xè½´æ ‡ç­¾
plt.ylabel('TPR') # yè½´æ ‡ç­¾
plt.grid(b=True)
plt.legend(loc='lower right', fancybox=True, framealpha=0.8, fontsize=14)
plt.title('è‡ªåŠ¨-ROCæ›²çº¿å’ŒAUCå€¼', fontsize=17)

plt.show()